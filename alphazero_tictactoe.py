# -*- coding: utf-8 -*-
"""AlphaZero TicTacToe

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RR1u56dt7P5E2QIP8AAil7yF-P70xLKk
"""

#**two seperate componenets**
#one hand we have self play right here **Alpha zero models play basically with itself** in order to gather information about the game
#while playing the data is generated which would be used during training of the data
#training phase we would optimize our model
#repeat n number of times until we have reached an array network 
#then it would become better than any other human also
#f(state)=(policy,value)
#policy here is a distribution and for each action it will tell us how promising, tell us where to play on the basis of state right here, value tells how promising state is
#We arrive at root childeren on the basis of action taken by root node
#w1 is the total number of wins which we have achieved due to certain action and n1 is the total visit count
#Monte Carlo Search Tree **Through the tree we could found the action which looks more promising**
#Selection phase: we walk down until leaf node(we can expand it in another direction)**expansion** **Simulation** **Backpropagation**

#Alpha Monte Carlo Search Tree P term (policy is added probability of each potential child)

!pip install numpy

import numpy as np
np.__version__

class TicTacToe:
    def __init__(self):
        self.row_count = 3
        self.column_count = 3
        self.action_size = self.row_count * self.column_count #Tic-tac-toe has 9 squares, each of which can be either an X, and O, or empty. 
        
    def get_initial_state(self):#we will call at the beginning
        return np.zeros((self.row_count, self.column_count))
    
    def get_next_state(self, state, action, player):#the action would be just an integer between 0 and 8 **0 would be the corner uplift abd 8 corner downright
        row = action // self.column_count #encode the action into row and column so that we can use it inside our numpy array
        column = action % self.column_count
        state[row, column] = player
        return state
    
    def get_valid_moves(self, state):#tells what moves are actually legal
        return (state.reshape(-1) == 0).astype(np.uint8)
    
    def check_win(self, state, action):
        row = action // self.column_count
        column = action % self.column_count
        player = state[row, column]
        
        return (
            np.sum(state[row, :]) == player * self.column_count
            or np.sum(state[:, column]) == player * self.row_count
            or np.sum(np.diag(state)) == player * self.row_count
            or np.sum(np.diag(np.flip(state, axis=0))) == player * self.row_count
        )
    
    def get_value_and_terminated(self, state, action):
        if self.check_win(state, action):
            return 1, True
        if np.sum(self.get_valid_moves(state)) == 0:
            return 0, True
        return 0, False
    
    def get_opponent(self, player):
        return -player

tictactoe = TicTacToe()
player = 1

state = tictactoe.get_initial_state()


while True:
    print(state)
    valid_moves = tictactoe.get_valid_moves(state)
    print("valid_moves", [i for i in range(tictactoe.action_size) if valid_moves[i] == 1])
    action = int(input(f"{player}:"))
    
    if valid_moves[action] == 0:
        print("action not valid")
        continue
        
    state = tictactoe.get_next_state(state, action, player)
    
    value, is_terminal = tictactoe.get_value_and_terminated(state, action)
    
    if is_terminal:
        print(state)
        if value == 1:
            print(player, "won")
        else:
            print("draw")
        break
        
    player = tictactoe.get_opponent(player)

